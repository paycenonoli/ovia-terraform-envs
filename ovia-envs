DevOps interview question: How do yu manage multiple environments in Terraform?
ChatGPT said:

Good question ğŸ‘.
When asked â€œHow do you manage multiple environments in Terraform?â€, interviewers want to see if you understand Terraform best practices for separating environments like dev, staging, and prod while keeping code reusable, secure, and consistent.

Here are the main approaches:

1. Workspaces

Terraform has the concept of workspaces (terraform workspace new dev, terraform workspace select prod).

Each workspace maintains its own state file.

Same configuration, different environment.

Best for small projects or when infra differences between environments are minimal.

2. Directory Structure (Separate Folders per Environment)

Example:

â”œâ”€â”€ environments
â”‚   â”œâ”€â”€ dev
â”‚   â”‚   â””â”€â”€ main.tf
â”‚   â”œâ”€â”€ staging
â”‚   â”‚   â””â”€â”€ main.tf
â”‚   â””â”€â”€ prod
â”‚       â””â”€â”€ main.tf


Each environment has its own Terraform code and separate state backend.

More explicit, easier to control environment-specific configurations.

Best for medium to large projects.

3. Modules + Environment-Specific Variables

Create reusable modules (e.g., vpc, eks, ec2) and keep environments as wrappers.

Example:

â”œâ”€â”€ modules
â”‚   â””â”€â”€ vpc
â”‚       â””â”€â”€ main.tf
â”œâ”€â”€ environments
â”‚   â”œâ”€â”€ dev
â”‚   â”‚   â””â”€â”€ main.tf (calls modules/vpc with dev vars)
â”‚   â”œâ”€â”€ staging
â”‚   â””â”€â”€ prod


Use variable files (dev.tfvars, prod.tfvars) to supply different values.

Then run:

terraform apply -var-file=dev.tfvars
terraform apply -var-file=prod.tfvars

4. Separate State Backends

Always use different remote backends (e.g., S3 buckets with different prefixes) for dev/staging/prod to avoid conflicts.

Example in backend.tf:

key = "prod/terraform.tfstate"

5. CI/CD Pipelines

Environments can be managed via pipelines (Jenkins, GitHub Actions, GitLab CI).

Pipeline selects the right backend + tfvars depending on the branch:

main â†’ prod

develop â†’ dev

release/* â†’ staging

âœ… Best Practice in Real World:

Use modules for reusability.

Use separate state backends per environment.

Drive deployments via CI/CD pipelines with environment-specific variables.

ğŸ‘‰ Would you like me to give you a sample folder structure + command flow that would impress an interviewer?

You said:
To clearly understand these concepts, I need a demo each, for the various approaches: Workspaces, Directory Structure (Separate Folders per Environment), Modules + Environment-Specific Variables and CI/CD Pipelines
ChatGPT said:

Perfect ğŸ‘ â€” letâ€™s do this step by step demos for each approach so you can clearly see how they work. Iâ€™ll keep the examples simple with an AWS S3 bucket so you can run them quickly and not worry about big infra.

Demo 1: Workspaces

ğŸ‘‰ Use workspaces when you want one codebase but separate state for each environment.

Code (main.tf)
provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "demo" {
  bucket = "demo-bucket-${terraform.workspace}"
}

Commands
terraform init
terraform workspace new dev
terraform apply -auto-approve
terraform workspace new prod
terraform apply -auto-approve


Creates demo-bucket-dev and demo-bucket-prod.

One codebase, multiple workspaces.

Demo 2: Directory Structure (Separate Folders per Environment)

ğŸ‘‰ Use different folders when environments need slightly different configs.

Structure
terraform-demo/
â”œâ”€â”€ dev/
â”‚   â””â”€â”€ main.tf
â”œâ”€â”€ prod/
â”‚   â””â”€â”€ main.tf

dev/main.tf
provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "demo" {
  bucket = "demo-bucket-dev"
}

prod/main.tf
provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "demo" {
  bucket = "demo-bucket-prod"
}

Commands
cd dev && terraform init && terraform apply -auto-approve
cd ../prod && terraform init && terraform apply -auto-approve


Clear separation â€” different state and configs per folder.

Demo 3: Modules + Environment-Specific Variables

ğŸ‘‰ Use modules for reusability, with tfvars for environment differences.

Structure
terraform-demo/
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ s3/
â”‚       â””â”€â”€ main.tf
â”œâ”€â”€ main.tf
â”œâ”€â”€ dev.tfvars
â”œâ”€â”€ prod.tfvars

modules/s3/main.tf
variable "bucket_name" {}

resource "aws_s3_bucket" "demo" {
  bucket = var.bucket_name
}

main.tf
provider "aws" {
  region = "us-east-1"
}

module "s3" {
  source      = "./modules/s3"
  bucket_name = var.bucket_name
}

variable "bucket_name" {}

dev.tfvars
bucket_name = "demo-bucket-dev"

prod.tfvars
bucket_name = "demo-bucket-prod"

Commands
terraform init
terraform apply -var-file=dev.tfvars -auto-approve
terraform apply -var-file=prod.tfvars -auto-approve


Reuses the same module, just swaps variables.

Demo 4: CI/CD Pipeline Approach

ğŸ‘‰ Automate deployments using pipelines. (Example: GitHub Actions)

Structure
terraform-demo/
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ s3/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ dev.tfvars
â”‚   â””â”€â”€ prod.tfvars
â”œâ”€â”€ main.tf
â”œâ”€â”€ .github/workflows/terraform.yml

.github/workflows/terraform.yml
name: Terraform Deploy

on:
  push:
    branches:
      - dev
      - prod

jobs:
  terraform:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2

      - name: Terraform Init
        run: terraform init

      - name: Terraform Apply (Dev)
        if: github.ref == 'refs/heads/dev'
        run: terraform apply -var-file=envs/dev.tfvars -auto-approve

      - name: Terraform Apply (Prod)
        if: github.ref == 'refs/heads/prod'
        run: terraform apply -var-file=envs/prod.tfvars -auto-approve


Push to dev branch â†’ deploys dev infra.

Push to prod branch â†’ deploys prod infra.

âœ… Thatâ€™s 4 clear demos showing all strategies.

Workspaces â†’ simple, one codebase.

Directories â†’ clear separation.

Modules + tfvars â†’ reusable, best practice.

CI/CD â†’ automated, real-world.

Would you like me to bundle all these demos into a single repo-style layout (so you can git clone and run each one separately)?

You said:
Now, please...repeat the demo using EC2 instance instead of S3 bucket, then Jenkins pipeline, instead of GitHub Actions.
ChatGPT said:

Perfect ğŸ’¯ â€” letâ€™s repeat the 4 demos using EC2 instance instead of S3 bucket, and for the CI/CD part weâ€™ll use a Jenkins pipeline instead of GitHub Actions.

For simplicity, weâ€™ll launch a t2.micro EC2 instance in us-east-1.

Demo 1: Workspaces (EC2)
Code (main.tf)
provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "demo" {
  ami           = "ami-08c40ec9ead489470" # Amazon Linux 2 AMI in us-east-1
  instance_type = "t2.micro"
  tags = {
    Name = "demo-ec2-${terraform.workspace}"
  }
}

Commands
terraform init
terraform workspace new dev
terraform apply -auto-approve   # EC2 with tag Name=demo-ec2-dev

terraform workspace new prod
terraform apply -auto-approve   # EC2 with tag Name=demo-ec2-prod

Demo 2: Directory Structure (Separate Folders per Environment)
Structure
terraform-demo/
â”œâ”€â”€ dev/
â”‚   â””â”€â”€ main.tf
â”œâ”€â”€ prod/
â”‚   â””â”€â”€ main.tf

dev/main.tf
provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "demo" {
  ami           = "ami-08c40ec9ead489470"
  instance_type = "t2.micro"
  tags = {
    Name = "demo-ec2-dev"
  }
}

prod/main.tf
provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "demo" {
  ami           = "ami-08c40ec9ead489470"
  instance_type = "t2.micro"
  tags = {
    Name = "demo-ec2-prod"
  }
}

Commands
cd dev && terraform init && terraform apply -auto-approve
cd ../prod && terraform init && terraform apply -auto-approve

Demo 3: Modules + Environment-Specific Variables
Structure
terraform-demo/
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ ec2/
â”‚       â””â”€â”€ main.tf
â”œâ”€â”€ main.tf
â”œâ”€â”€ dev.tfvars
â”œâ”€â”€ prod.tfvars

modules/ec2/main.tf
variable "instance_name" {}

resource "aws_instance" "demo" {
  ami           = "ami-08c40ec9ead489470"
  instance_type = "t2.micro"
  tags = {
    Name = var.instance_name
  }
}

main.tf
provider "aws" {
  region = "us-east-1"
}

module "ec2" {
  source        = "./modules/ec2"
  instance_name = var.instance_name
}

variable "instance_name" {}

dev.tfvars
instance_name = "demo-ec2-dev"

prod.tfvars
instance_name = "demo-ec2-prod"

Commands
terraform init
terraform apply -var-file=dev.tfvars -auto-approve
terraform apply -var-file=prod.tfvars -auto-approve

Demo 4: CI/CD with Jenkins Pipeline

ğŸ‘‰ Jenkins will trigger Terraform deployments based on branch name (similar to GitHub Actions).

Structure
terraform-demo/
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ ec2/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ dev.tfvars
â”‚   â””â”€â”€ prod.tfvars
â”œâ”€â”€ main.tf
â”œâ”€â”€ Jenkinsfile

Jenkinsfile
pipeline {
    agent any

    environment {
        AWS_DEFAULT_REGION = "us-east-1"
    }

    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Terraform Init') {
            steps {
                sh 'terraform init'
            }
        }

        stage('Terraform Apply Dev') {
            when {
                branch 'dev'
            }
            steps {
                sh 'terraform apply -var-file=envs/dev.tfvars -auto-approve'
            }
        }

        stage('Terraform Apply Prod') {
            when {
                branch 'prod'
            }
            steps {
                sh 'terraform apply -var-file=envs/prod.tfvars -auto-approve'
            }
        }
    }
}

Workflow

Push code to dev branch â†’ Jenkins runs pipeline â†’ deploys EC2 instance with tag demo-ec2-dev.

Push code to prod branch â†’ Jenkins runs pipeline â†’ deploys EC2 instance with tag demo-ec2-prod.

âœ… Now you have the same 4 demos but with EC2 instances and Jenkins pipeline instead of S3 and GitHub Actions.

Would you like me to also include remote backend (S3 + DynamoDB for locking) in these demos, so you can see how state isolation per environment works in real-world setups?

You said:
Yes, please
ChatGPT said:

Awesome ğŸš€ â€” letâ€™s extend all four demos to include a remote backend using S3 for state storage and DynamoDB for state locking (best practice in real-world Terraform setups).

This way:

S3 bucket stores Terraform state.

DynamoDB table ensures only one person/process modifies state at a time.

Each environment has its own state file (isolated).

Step 1: Setup Backend (once)

Before running any demo, create the backend infrastructure manually or with Terraform (best practice is Terraform).

backend-setup.tf
provider "aws" {
  region = "us-east-1"
}

resource "aws_s3_bucket" "tf_state" {
  bucket = "my-terraform-state-bucket-payce"
  acl    = "private"

  versioning {
    enabled = true
  }
}

resource "aws_dynamodb_table" "tf_lock" {
  name         = "terraform-lock-table"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }
}


Apply it once:

terraform init
terraform apply -auto-approve


Now you have:

S3 bucket: my-terraform-state-bucket-payce

DynamoDB table: terraform-lock-table

Step 2: Add Backend Config to Each Demo

You add the same backend block to each environment or workspace config.

backend.tf
terraform {
  backend "s3" {
    bucket         = "my-terraform-state-bucket-payce"
    key            = "envs/dev/terraform.tfstate" # ğŸ‘ˆ changes per env/workspace
    region         = "us-east-1"
    dynamodb_table = "terraform-lock-table"
    encrypt        = true
  }
}

Demo Updates with Backend
Demo 1: Workspaces

backend.tf

terraform {
  backend "s3" {
    bucket         = "my-terraform-state-bucket-payce"
    key            = "workspaces/${terraform.workspace}/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-lock-table"
    encrypt        = true
  }
}


State stored in s3://my-terraform-state-bucket-payce/workspaces/dev/terraform.tfstate

Automatically separates by workspace.

Demo 2: Directory Structure

dev/backend.tf

terraform {
  backend "s3" {
    bucket         = "my-terraform-state-bucket-payce"
    key            = "dev/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-lock-table"
    encrypt        = true
  }
}


prod/backend.tf

terraform {
  backend "s3" {
    bucket         = "my-terraform-state-bucket-payce"
    key            = "prod/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-lock-table"
    encrypt        = true
  }
}


Each folder has its own state file.

Demo 3: Modules + tfvars

backend.tf

terraform {
  backend "s3" {
    bucket         = "my-terraform-state-bucket-payce"
    key            = "shared/terraform.tfstate" # will override with -backend-config
    region         = "us-east-1"
    dynamodb_table = "terraform-lock-table"
    encrypt        = true
  }
}


When applying:

terraform init -backend-config="key=dev/terraform.tfstate"
terraform apply -var-file=dev.tfvars -auto-approve

terraform init -backend-config="key=prod/terraform.tfstate"
terraform apply -var-file=prod.tfvars -auto-approve

Demo 4: Jenkins Pipeline

Modify Jenkinsfile to pass backend key per branch.

pipeline {
    agent any

    environment {
        AWS_DEFAULT_REGION = "us-east-1"
    }

    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Terraform Init') {
            when { branch 'dev' }
            steps {
                sh 'terraform init -backend-config="key=dev/terraform.tfstate"'
            }
            when { branch 'prod' }
            steps {
                sh 'terraform init -backend-config="key=prod/terraform.tfstate"'
            }
        }

        stage('Terraform Apply Dev') {
            when { branch 'dev' }
            steps {
                sh 'terraform apply -var-file=envs/dev.tfvars -auto-approve'
            }
        }

        stage('Terraform Apply Prod') {
            when { branch 'prod' }
            steps {
                sh 'terraform apply -var-file=envs/prod.tfvars -auto-approve'
            }
        }
    }
}


âœ… End Result:

Workspaces â†’ backend key per workspace

Directories â†’ backend key per folder

Modules + tfvars â†’ backend key chosen at init

Jenkins â†’ backend key chosen per branch

ğŸ‘‰ Do you want me to also show you how to switch between environments safely in Jenkins (using parameters instead of branch names), so you can manually promote from dev â†’ prod inside Jenkins?

You said:
Yes
ChatGPT said:

Perfect! Letâ€™s upgrade your Jenkins pipeline to allow manual promotion between environments instead of relying solely on branch names. This is very common in real-world DevOps workflows.

Weâ€™ll use Jenkins parameters to choose the environment (dev, staging, prod) when triggering the pipeline.

Jenkins Pipeline with Manual Environment Selection
Jenkinsfile
pipeline {
    agent any

    parameters {
        choice(
            name: 'ENV',
            choices: ['dev', 'staging', 'prod'],
            description: 'Select the environment to deploy'
        )
    }

    environment {
        AWS_DEFAULT_REGION = "us-east-1"
    }

    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Terraform Init') {
            steps {
                script {
                    def backendKey = "${params.ENV}/terraform.tfstate"
                    sh "terraform init -backend-config='key=${backendKey}'"
                }
            }
        }

        stage('Terraform Plan') {
            steps {
                script {
                    sh "terraform plan -var-file=envs/${params.ENV}.tfvars"
                }
            }
        }

        stage('Terraform Apply') {
            steps {
                script {
                    sh "terraform apply -var-file=envs/${params.ENV}.tfvars -auto-approve"
                }
            }
        }
    }

    post {
        success {
            echo "Terraform applied successfully to ${params.ENV} environment."
        }
        failure {
            echo "Terraform deployment to ${params.ENV} failed!"
        }
    }
}

How It Works

When you click Build with Parameters in Jenkins:

You can choose dev, staging, or prod.

The pipeline dynamically:

Sets the S3 backend key: dev/terraform.tfstate, staging/terraform.tfstate, or prod/terraform.tfstate.

Uses the corresponding tfvars file: envs/dev.tfvars, etc.

Terraform plan and apply run for the selected environment.

State isolation is maintained per environment via the S3 backend.

DynamoDB prevents concurrent changes.

Advantages

Safe promotion from dev â†’ staging â†’ prod without touching branches.

Single Jenkinsfile handles all environments.

Environment-specific variables and state isolation are fully maintained.

Manual approval can be added before apply for extra safety.

If you want, I can also enhance this pipeline with a â€œManual Approvalâ€ stage before applying to staging/prod, which is a standard best practice in enterprise pipelines.

Do you want me to do that?

You said:
Yes, please
ChatGPT said:

Perfect ğŸ˜ â€” letâ€™s add a manual approval stage before Terraform applies to staging or prod, so deployments can be safely reviewed before execution.

Enhanced Jenkinsfile with Manual Approval
pipeline {
    agent any

    parameters {
        choice(
            name: 'ENV',
            choices: ['dev', 'staging', 'prod'],
            description: 'Select the environment to deploy'
        )
    }

    environment {
        AWS_DEFAULT_REGION = "us-east-1"
    }

    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Terraform Init') {
            steps {
                script {
                    def backendKey = "${params.ENV}/terraform.tfstate"
                    sh "terraform init -backend-config='key=${backendKey}'"
                }
            }
        }

        stage('Terraform Plan') {
            steps {
                script {
                    sh "terraform plan -var-file=envs/${params.ENV}.tfvars"
                }
            }
        }

        stage('Approval') {
            when {
                anyOf {
                    expression { params.ENV == 'staging' }
                    expression { params.ENV == 'prod' }
                }
            }
            steps {
                timeout(time: 30, unit: 'MINUTES') {
                    input message: "Approve deployment to ${params.ENV}?"
                }
            }
        }

        stage('Terraform Apply') {
            steps {
                script {
                    sh "terraform apply -var-file=envs/${params.ENV}.tfvars -auto-approve"
                }
            }
        }
    }

    post {
        success {
            echo "Terraform applied successfully to ${params.ENV} environment."
        }
        failure {
            echo "Terraform deployment to ${params.ENV} failed!"
        }
    }
}

How It Works

Parameter selection: You pick dev, staging, or prod.

Init & Plan: Terraform initializes backend and runs plan.

Manual Approval Stage:

Triggered only for staging or prod.

Jenkins pauses and waits for a human to approve.

Timeout ensures the pipeline doesnâ€™t hang indefinitely.

Apply: Runs terraform apply only after approval (or immediately for dev).

State isolation is still maintained via S3 + DynamoDB.

Advantages

Prevents accidental production deployment.

Maintains single pipeline for all environments.

Keeps Terraform state safe and isolated.

Can integrate Slack/email notifications for approval alerts.

If you want, I can draw a full diagram showing all 4 approaches (Workspaces, Directories, Modules, Jenkins with Approval) with remote state isolation, so you have a visual map for interview explanations.

